{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os, h5py\n",
    "from sgg_benchmark.modeling.detector import build_detection_model\n",
    "from sgg_benchmark.config import cfg\n",
    "\n",
    "config_file = '/home/maelic/Documents/PhD/MyModel/Scene-Graph-Benchmark-Cuda11.7/configs/VG150/curated/e2e_relation_X_101_32_8_FPN_1x_custom_pred.yaml'\n",
    "path = \"/home/maelic/Documents/Scene-Graph-Benchmark-Cuda11.7/datasets/VG150/curated/VG-SGG-with-attri.h5\"\n",
    "# load with h5py\n",
    "data = h5py.File(path, 'r')\n",
    "# load image data\n",
    "image_data = json.load(open(\"/home/maelic/Documents/PhD/MyModel/Scene-Graph-Benchmark-Cuda11.7/datasets/vg/image_data.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32422\n"
     ]
    }
   ],
   "source": [
    "# get images ids from test split\n",
    "test_ids = []\n",
    "for i, img in enumerate(data['split_rel']):\n",
    "    if img == 2:\n",
    "        id = image_data[i]['image_id']\n",
    "        test_ids.append(id)\n",
    "print(len(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 14:31:00.400 | INFO     | sgg_benchmark.data.build:get_dataset_statistics:30 - ----------------------------------------------------------------------------------------------------\n",
      "2023-03-04 14:31:00.400 | INFO     | sgg_benchmark.data.build:get_dataset_statistics:31 - get dataset statistics...\n",
      "2023-03-04 14:31:00.401 | INFO     | sgg_benchmark.data.build:get_dataset_statistics:42 - Loading data statistics from: /home/maelic/Documents/Scene-Graph-Benchmark-Cuda11.7/checkpoints/VG150/curated/sgdet_motifs_causal_tde/VG150_curated_filtered_train_statistics.cache\n",
      "2023-03-04 14:31:00.401 | INFO     | sgg_benchmark.data.build:get_dataset_statistics:43 - ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word vectors from /home/maelic/glove/glove.6B.200d.pt\n",
      "__background__ -> __background__ \n",
      "fail on __background__\n",
      "loading word vectors from /home/maelic/glove/glove.6B.200d.pt\n",
      "__background__ -> __background__ \n",
      "fail on __background__\n",
      "64\n",
      "3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unsupported type for to_image_list: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m dummy_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(N,C)\n\u001b[1;32m     13\u001b[0m dummy_input \u001b[39m=\u001b[39m dummy_input[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m,:, \u001b[39mNone\u001b[39;00m,\u001b[39mNone\u001b[39;00m] \u001b[39m#adding the None for height and weight\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(model, \u001b[39m\"\u001b[39;49m\u001b[39msgdet_motifs_tde.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m, dummy_input)\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.8/site-packages/torch/onnx/__init__.py:350\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39mExports a model into ONNX format. If ``model`` is not a\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m:class:`torch.jit.ScriptModule` nor a :class:`torch.jit.ScriptFunction`, this runs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39m    model to the file ``f`` even if this is raised.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[0;32m--> 350\u001b[0m \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39;49mexport(\n\u001b[1;32m    351\u001b[0m     model,\n\u001b[1;32m    352\u001b[0m     args,\n\u001b[1;32m    353\u001b[0m     f,\n\u001b[1;32m    354\u001b[0m     export_params,\n\u001b[1;32m    355\u001b[0m     verbose,\n\u001b[1;32m    356\u001b[0m     training,\n\u001b[1;32m    357\u001b[0m     input_names,\n\u001b[1;32m    358\u001b[0m     output_names,\n\u001b[1;32m    359\u001b[0m     operator_export_type,\n\u001b[1;32m    360\u001b[0m     opset_version,\n\u001b[1;32m    361\u001b[0m     do_constant_folding,\n\u001b[1;32m    362\u001b[0m     dynamic_axes,\n\u001b[1;32m    363\u001b[0m     keep_initializers_as_inputs,\n\u001b[1;32m    364\u001b[0m     custom_opsets,\n\u001b[1;32m    365\u001b[0m     export_modules_as_functions,\n\u001b[1;32m    366\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.8/site-packages/torch/onnx/utils.py:163\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    146\u001b[0m     model,\n\u001b[1;32m    147\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m     export_modules_as_functions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    161\u001b[0m ):\n\u001b[0;32m--> 163\u001b[0m     _export(\n\u001b[1;32m    164\u001b[0m         model,\n\u001b[1;32m    165\u001b[0m         args,\n\u001b[1;32m    166\u001b[0m         f,\n\u001b[1;32m    167\u001b[0m         export_params,\n\u001b[1;32m    168\u001b[0m         verbose,\n\u001b[1;32m    169\u001b[0m         training,\n\u001b[1;32m    170\u001b[0m         input_names,\n\u001b[1;32m    171\u001b[0m         output_names,\n\u001b[1;32m    172\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    173\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    174\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    175\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    176\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    177\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    178\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    179\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.8/site-packages/torch/onnx/utils.py:1074\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1071\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1072\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1074\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[1;32m   1075\u001b[0m     model,\n\u001b[1;32m   1076\u001b[0m     args,\n\u001b[1;32m   1077\u001b[0m     verbose,\n\u001b[1;32m   1078\u001b[0m     input_names,\n\u001b[1;32m   1079\u001b[0m     output_names,\n\u001b[1;32m   1080\u001b[0m     operator_export_type,\n\u001b[1;32m   1081\u001b[0m     val_do_constant_folding,\n\u001b[1;32m   1082\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1083\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1084\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1085\u001b[0m )\n\u001b[1;32m   1087\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[1;32m   1089\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39monnx\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1090\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.8/site-packages/torch/onnx/utils.py:727\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m    724\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m    726\u001b[0m model \u001b[39m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m--> 727\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args)\n\u001b[1;32m    728\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m    730\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.8/site-packages/torch/onnx/utils.py:602\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 602\u001b[0m     graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[1;32m    603\u001b[0m     _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    604\u001b[0m     state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.8/site-packages/torch/onnx/utils.py:517\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_trace_and_get_graph_from_model\u001b[39m(model, args):\n\u001b[1;32m    513\u001b[0m     \u001b[39m# A basic sanity check: make sure the state_dict keys are the same\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     \u001b[39m# before and after running the model.  Fail fast!\u001b[39;00m\n\u001b[1;32m    515\u001b[0m     orig_state_dict_keys \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\u001b[39m.\u001b[39mkeys()\n\u001b[0;32m--> 517\u001b[0m     trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_get_trace_graph(\n\u001b[1;32m    518\u001b[0m         model, args, strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, _force_outplace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, _return_inputs_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m     warn_on_static_input_change(inputs_states)\n\u001b[1;32m    522\u001b[0m     \u001b[39mif\u001b[39;00m orig_state_dict_keys \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.8/site-packages/torch/jit/_trace.py:1175\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1174\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m-> 1175\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.8/site-packages/torch/jit/_trace.py:127\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 127\u001b[0m graph, out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_create_graph_by_tracing(\n\u001b[1;32m    128\u001b[0m     wrapper,\n\u001b[1;32m    129\u001b[0m     in_vars \u001b[39m+\u001b[39;49m module_state,\n\u001b[1;32m    130\u001b[0m     _create_interpreter_name_lookup_fn(),\n\u001b[1;32m    131\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrict,\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_force_outplace,\n\u001b[1;32m    133\u001b[0m )\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs:\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, outs[\u001b[39m0\u001b[39m], ret_inputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.8/site-packages/torch/jit/_trace.py:118\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    117\u001b[0m     inputs_states\u001b[39m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 118\u001b[0m outs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(\u001b[39m*\u001b[39;49mtrace_inputs))\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    120\u001b[0m     inputs_states[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m (inputs_states[\u001b[39m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.8/site-packages/torch/nn/modules/module.py:1118\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1118\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1119\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1120\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Documents/PhD/MyModel/Scene-Graph-Benchmark-Cuda11.7/sgg_benchmark/modeling/detector/generalized_rcnn.py:48\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets, logger)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mand\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIn training mode, targets should be passed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m images \u001b[39m=\u001b[39m to_image_list(images)\n\u001b[1;32m     49\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackbone(images\u001b[39m.\u001b[39mtensors)\n\u001b[1;32m     50\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrpn(images, features, targets)\n",
      "File \u001b[0;32m~/Documents/PhD/MyModel/Scene-Graph-Benchmark-Cuda11.7/sgg_benchmark/structures/image_list.py:72\u001b[0m, in \u001b[0;36mto_image_list\u001b[0;34m(tensors, size_divisible)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m ImageList(batched_imgs, image_sizes)\n\u001b[1;32m     71\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnsupported type for to_image_list: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(tensors)))\n",
      "\u001b[0;31mTypeError\u001b[0m: Unsupported type for to_image_list: <class 'str'>"
     ]
    }
   ],
   "source": [
    "cfg.merge_from_file(config_file)\n",
    "\n",
    "model = build_detection_model(cfg)\n",
    "model.to(cfg.MODEL.DEVICE)\n",
    "dummy_input = torch.randn(1, 3, 600, 800)\n",
    "shape_of_first_layer = list(model.parameters())[0].shape #shape_of_first_layer\n",
    "\n",
    "N,C = shape_of_first_layer[:2]\n",
    "print(N)\n",
    "print(C)\n",
    "dummy_input = torch.Tensor(N,C)\n",
    "\n",
    "dummy_input = dummy_input[...,:, None,None] #adding the None for height and weight\n",
    "torch.onnx.export(model, \"sgdet_motifs_tde.onnx\", dummy_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 7, 7])\n",
      "torch.Size([3, 600, 800])\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters())[0].shape)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "results_dict = {}\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "from torchvision.io import read_image\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    image = read_image(\"/home/maelic/Documents/PhD/Datasets/VisualGenome/VG_100K/10000.jpg\")\n",
    "\n",
    "    output = model(image.to(device), targets)\n",
    "\n",
    "    output = [o.to(cpu_device) for o in output]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7938123ceb831a528202b02dbdb8fd920084013224d6ffd4fcb9a5f6f5074515"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
